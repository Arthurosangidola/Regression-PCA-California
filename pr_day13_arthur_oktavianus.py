# -*- coding: utf-8 -*-
"""PR_day13_Arthur Oktavianus.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l4TNt3h0YUA9fs1hpIGMen4ry7SHUlC7

### Poin-poin Assignment
1. Pilih Dataset House Price California (california_dataset.csv) atau dataset House Price (house_price.csv) Untuk kasus Regresi -> Pilih salah satu dataset saja (Wajib dikerjakan) : https://drive.google.com/drive/folders/19eH6N3Oj0byCYORqA0Zq90SunomYFuOe?usp=sharing
2. Jika ingin memilih dataset klasifikasi (dataset_1.csv) dengan kolom target bernama target, lakukan PCA dan bandingkan modelling tanpa PCA dan dengan PCA.
"""

import warnings
warnings.filterwarnings('ignore')
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

#Model ML Regresi
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor

#Evaluation Regresi -> tambahkan matrix yang lain jika dibutuhkan
from sklearn.metrics import mean_squared_error


#import model classification
from sklearn.linear_model import LogisticRegression #logistic regression
from sklearn.svm import SVC #SVM
from sklearn.naive_bayes import GaussianNB #Naive Bayes
from sklearn.neighbors import KNeighborsClassifier #KNN
#import for metrics evaluasi klasifikasi
from sklearn.metrics import precision_score, \
    recall_score, confusion_matrix, classification_report, \
    accuracy_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,classification_report,accuracy_score

from google.colab import drive
drive.mount('/content/drive')

# california_dataset : Regresi

df1 = pd.read_csv("/content/drive/MyDrive/Dibimbing - AI ML/Day 13/Assignment/california_dataset.csv")
df1.shape

"""### Assignment 1
Dari 3 dataset diatas, pilih salah satu saja
1. Pahami dataset yang dipilih dengan cek dataset menggunakan : info(), sample(5), head(), tail() dan melihat value counts di tiap-tiap kolom.

## Pengenalan Umum atas Struktur Dataset

*   Menampilkan informasi tentang data
*   Menampilkan contoh baris acak, lima baris pertama, dan lima baris terakhir
*   Menampilkan jumlah nilai unik dalam setiap kolom

## Attribute Information:
1. MedianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars)

2. HousingAge: Age of a house within a block; a lower number is a newer building

3. AverageRooms: Average number of rooms within a block

4. AverageBedrooms: Average number of bedrooms within a block

5. population: Total number of people residing within a block

6. AverageOccupation: Average number of people residing in a household in a census block

7. Latitude: A measure of how far north a house is; a higher value is farther north

8. Longitude: A measure of how far west a house is; a higher value is farther west

9. Houseprice: House prices in the census block

### Menampilkan informasi tentang data
"""

print("Informasi tentang data:")
print(df1.info())

"""Sekilas jika melihat output data diatas ada beberapa informasi yang kita dapati, yaitu:

1.   Jumlah banyaknya row data atau baris yaitu sebanyak 20.640
2.   Semua tipe data berbentuk float
3.   Jumlah column ada 9, yaitu 8 feature dan 1 target
4.   Sekilas jika melihat dari output diatas setiap column jumlah barisnya sama, sehingga disimpulkan tidak ada *missing value*

### Menampilkan contoh baris acak
"""

print("\nContoh 5 baris acak:")
print(df1.sample(5))

"""Output di atas merupakan 5 baris acak (sample) dari data frame california_dataset

### Menampilkan lima baris pertama
"""

print("\nLima baris pertama:")
print(df1.head())

"""### Menampilkan lima baris terakhir"""

print("\nLima baris terakhir:")
print(df1.tail())

"""### Menghitung jumlah nilai unik dalam setiap kolom"""

for col in df1.columns:
    print(f"\nValue counts untuk kolom '{col}':")
    print(df1[col].value_counts())

"""## As the **longitude and latitude** features will not be used in the analysis, we have removed those features"""

df1.drop(['Latitude', 'Longitude'], axis=1, inplace=True)

"""### Assignment 2 : Lakukan EDA untuk kolom numerik dan kategorikal, silahkan saudara eksplor.

## Exploratory Data Analysis (EDA)

Note: The consideration for conducting EDA on the Full Dataset or before data splitting is because:

1. Explore the entire dataset to understand its structure, distribution, and potential issues.
2. Identify outliers, examine relationships between variables, and make decisions about feature engineering.

#### **The df.describe()** method generates descriptive statistics for numerical columns in the DataFrame. This includes:
* Count: Number of non-null values in each column.
* Mean: Average value of each column.
* Std (Standard Deviation): Measure of the amount of variation or dispersion.
* Min and Max: Minimum and maximum values in each column.
* 25th, 50th (median), and 75th percentiles: Provide insights into the distribution of the data.
"""

df1.describe()

"""From the df1.describe() output, we can infer several insights about the data:
1. Outliers:
   AveRooms: The maximum value is substantially higher than the 75th percentile, suggesting potential outliers.
   AveBedrms: Similar to AveRooms, the maximum value is considerably higher than the 75th percentile.

2. Non-Normal Distribution:
   Population: The standard deviation is relatively high, indicating significant variability. The 75th percentile is far from the maximum, suggesting a right-skewed distribution with a potential presence of outliers.
   AveOccup: Similar to Population, a high standard deviation and a potential right-skewed distribution.

3. Data Ranges:
   house_price: The target variable, shows a wide range from the minimum to maximum, indicating variability in house prices.

4. Observations:
   The minimum and maximum values for some features (e.g., AveRooms, AveBedrms, Population, AveOccup) suggest a potential presence of outliers or extreme values.
"""

features = list(df1.columns)
features

"""Berdasarkan output data frame **features**, saat ini hanya ada 7 kolom, dimana 6 features dan 1 target"""

# Plotting
fig, ax = plt.subplots(len(features), 2, figsize=(10, 30))
for idx, column in enumerate(features):
    sns.distplot(x=df1[column], color='green', ax=ax[idx][0])
    ax[idx][0].set_title(f'Distribusi {column} (Skewness: {skewness[column]:.2f})')
    ax[idx][0].set_xlabel('Value')
    ax[idx][0].set_ylabel('Frequency')

    sns.boxplot(x=df1[column], color='blue', ax=ax[idx][1])
    ax[idx][1].set_title(f'Boxplot {column}')

    # Menambahkan rentang angka di dalam chart box plot
    q1 = df1[column].quantile(0.25)
    q3 = df1[column].quantile(0.75)
    # ax[idx][1].axvline(q1, color='red', linestyle='--', label='Q1')
    # ax[idx][1].axvline(q3, color='blue', linestyle='--', label='Q3')

    # Menambahkan rentang angka di dalam chart bar
    ax[idx][0].axvline(q1, color='red', linestyle='--', label='Q1')
    ax[idx][0].axvline(q3, color='blue', linestyle='--', label='Q3')

    ax[idx][0].legend()

plt.tight_layout()
plt.show()

# Menampilkan informasi rentang Q1 dan Q3 untuk setiap kolom
print("Rentang antara Q1 dan Q3 untuk Setiap Kolom:")
for column in q1_q3_range.index:
    print(f"{column} = berada di antara rentang {q1[column]:.2f}-{q3[column]:.2f}")

# Menghitung skewness untuk setiap variabel numerikal
skewness = df1[features].apply(lambda x: skew(x))
print("Skewness untuk setiap variabel numerikal:")
print(skewness)

"""### Analisis dan insight tentang distribusi dan variasi data dalam setiap fitur

1. Skewness (Keserongan) untuk Setiap Variabel Numerikal:
   * MedInc memiliki skewness sebesar 1.65, menunjukkan bahwa distribusinya cenderung miring ke kanan (positif). Hal ini menandakan bahwa kebanyakan rumah memiliki pendapatan median yang rendah, tetapi ada beberapa rumah dengan pendapatan yang sangat tinggi.
        
   * HouseAge memiliki skewness yang sangat kecil, mendekati nol, menunjukkan bahwa distribusinya hampir simetris atau tidak miring.
        
   * AveRooms dan AveBedrms memiliki skewness yang sangat tinggi, masing-masing 20.70 dan 31.31, menunjukkan bahwa distribusi keduanya sangat miring ke kanan (positif). Ini mungkin menunjukkan adanya beberapa titik data yang jauh di atas rata-rata.
        
   * Population memiliki skewness sebesar 4.94, menunjukkan distribusi yang sangat miring ke kanan (positif). Ini menunjukkan bahwa sebagian besar area memiliki populasi yang rendah, tetapi ada beberapa area dengan populasi yang sangat tinggi.
        
   * AveOccup memiliki skewness yang sangat tinggi, sebesar 97.63, menunjukkan distribusi yang sangat miring ke kanan (positif). Hal ini menandakan bahwa sebagian besar area memiliki tingkat hunian yang rendah, tetapi ada beberapa area dengan tingkat hunian yang sangat tinggi.
        
   * house_price memiliki skewness sebesar 0.98, menunjukkan sedikit kemiringan ke kanan (positif). Ini menandakan bahwa sebagian besar harga rumah cenderung rendah, tetapi ada beberapa rumah dengan harga yang lebih tinggi.

2. Rentang antara Q1 dan Q3 untuk Setiap Kolom:
   * Rentang antara Q1 dan Q3 untuk MedInc adalah 2.56 hingga 4.74. Ini menunjukkan bahwa 50% dari data MedInc berkisar antara 2.56 hingga 4.74.
        
   * Rentang antara Q1 dan Q3 untuk HouseAge adalah 18.00 hingga 37.00. Ini menunjukkan bahwa 50% dari data HouseAge berkisar antara 18.00 hingga 37.00 tahun.
   
   * Rentang antara Q1 dan Q3 untuk AveRooms adalah 4.44 hingga 6.05. Ini menunjukkan bahwa 50% dari data AveRooms berkisar antara 4.44 hingga 6.05.
   
   * Rentang antara Q1 dan Q3 untuk AveBedrms adalah 1.01 hingga 1.10. Ini menunjukkan bahwa 50% dari data AveBedrms berkisar antara 1.01 hingga 1.10.
        
   * Rentang antara Q1 dan Q3 untuk Population adalah 787.00 hingga 1725.00. Ini menunjukkan bahwa 50% dari data Populasi berkisar antara 787.00 hingga 1725.00.
        
   * Rentang antara Q1 dan Q3 untuk AveOccup adalah 2.43 hingga 3.28. Ini menunjukkan bahwa 50% dari data AveOccup berkisar antara 2.43 hingga 3.28.
        
   * Rentang antara Q1 dan Q3 untuk house_price adalah 1.20 hingga 2.65. Ini menunjukkan bahwa 50% dari data house_price berkisar antara 1.20 hingga 2.65.


"""

X_feature = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup']

X_index = list(range(len(X_feature)))
X_index

X_feature = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup']
X_index = list(range(len(X_feature)))
fig, ax = plt.subplots(13,1,figsize=(9,14))
for i in X_index:
    sns.lineplot(data=df1, x = X_feature[i], y = 'house_price', ax=ax[i])
    plt.tight_layout()

"""### Assignment 3 : Lakukan Feature Egnineering
1. Lakukan teknik-teknik feature engineering dan analisa jika diperlukan teknik tertentu maka lakukan, jika tidak maka tuliskan alasannya. Misal tidak perlu missing value handling karena setelah dicek tidak ada missing value di dalam dataset.

### Feature Egnineering

1. Drop duplikat
2. Outlier Handling
3. Feature Scalling
4. Karena pada saat di EDA didapati informasi tidak ada missing value, maka pada Feature Engineering ini tidak dilakukan handling.

### 1. Cek dan Drop Duplikat
"""

# Drop Duplicates

print(f"Dataframe dimension before duplication drop {df1.shape[0]}")

#membuang duplikat data
df1 = df1.drop_duplicates().reset_index(drop=True)

print(f"Dataframe dimension after duplication drop {df1.shape[0]}")

"""### 2. Outlier Handling

Alasan pada case ini dilakukan outlier handling yaitu :
1. Outlier dapat memiliki pengaruh yang signifikan terhadap hasil PCA. Outlier dapat menyebabkan komponen utama menjadi tidak mewakili variasi sebenarnya dalam data, dan dapat memengaruhi interpretasi hasil PCA.

2. Outlier dapat menyebabkan model PCA menjadi tidak stabil. Ini dapat menghasilkan perbedaan yang signifikan dalam hasil PCA jika outlier hadir atau tidak hadir dalam data.

3. PCA diasumsikan data berdistribusi normal. Outlier dapat mengganggu asumsi ini dan menghasilkan komponen utama yang tidak mewakili struktur data dengan baik.

4. Meskipun StandardScaler dapat membantu dalam menangani perbedaan skala antar fitur, itu tidak mempengaruhi penanganan outlier. Outlier tetap menjadi masalah yang relevan bahkan setelah standarisasi data.
"""

# Menghitung Q1 dan Q3 untuk setiap fitur
q1 = df1.quantile(0.25)
q3 = df1.quantile(0.75)

# Menghitung rentang antara Q1 dan Q3 untuk setiap fitur
q1_q3_range = q3 - q1

# Menentukan batas atas dan batas bawah untuk mendefinisikan outlier
upper_bound = q3 + 1.5 * q1_q3_range
lower_bound = q1 - 1.5 * q1_q3_range

# Menghitung jumlah outlier di setiap kolom
outlier_count_per_column = ((df1 < lower_bound) | (df1 > upper_bound)).sum()

# Menghitung total jumlah data
total_data = len(df1)

# Menghitung persentase outlier di setiap kolom
outlier_percentage_per_column = (outlier_count_per_column / total_data) * 100

# Menampilkan jumlah outlier di setiap kolom
print("Jumlah outlier di setiap kolom:")
print(outlier_count_per_column)
print()

# Menampilkan persentase outlier di setiap kolom
print("Persentase outlier di setiap kolom:")
print(outlier_percentage_per_column)

"""Dalam kasus ini, penanganan outlier mungkin hanya diperlukan untuk fitur 'MedInc', 'AveRooms', 'AveBedrms', 'Population', dan 'AveOccup'"""

# Menghitung Q1 dan Q3 untuk setiap fitur
q1 = df1.quantile(0.25)
q3 = df1.quantile(0.75)

# Menghitung rentang antara Q1 dan Q3 untuk setiap fitur
q1_q3_range = q3 - q1

# Menentukan batas atas dan batas bawah untuk mendefinisikan outlier
upper_bound = q3 + 1.5 * q1_q3_range
lower_bound = q1 - 1.5 * q1_q3_range

# Handling outlier untuk kolom 'MedInc', 'AveRooms', dan 'AveOccup' dengan menghapus baris -> pertimbangan jumlah outlier tidak banyak
cleaned_df = df1.copy()
outlier_cols_drop = ['MedInc', 'AveRooms', 'AveOccup']
for col in outlier_cols_drop:
    if col in cleaned_df.columns:
        cleaned_df = cleaned_df[(cleaned_df[col] >= lower_bound[col]) & (cleaned_df[col] <= upper_bound[col])]

# Perbarui DataFrame setelah penghapusan outlier
cleaned_df.reset_index(drop=True, inplace=True)

# Handling outlier untuk kolom 'AveBedrms' dan 'Population' dengan mengganti nilai outlier dengan median -> pertimbangan jumlah outlier sangat banyak sehingga jika di drop akan mengurangi jumlah data signifikan
outlier_cols_median = ['AveBedrms', 'Population']
for col in outlier_cols_median:
    if col in cleaned_df.columns:
        median = cleaned_df[col].median()
        cleaned_df[col] = np.where((cleaned_df[col] < lower_bound[col]) | (cleaned_df[col] > upper_bound[col]), median, cleaned_df[col])

df = cleaned_df.dropna()

df.describe()

"""### Karena akan dilakukan PCA (Unsupervised) sehingga pada case ini tidak dilakukan di-splitting dahulu"""

X = df.drop('house_price', axis=1).values
y = df['house_price'].values

"""### 3. Feature Scalling"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X)
X_std = scaler.transform(X)

feats = ['MedInc',
 'HouseAge',
 'AveRooms',
 'AveBedrms',
 'Population',
 'AveOccup'
 ]

new_df = pd.DataFrame(data = X_std, columns = feats)
new_df.head()

new_df.describe()

"""### Assignment 4. Modelling PCA dengan libraries Sklearn atau from Scratch (Pilih salah satu saja)
1. Modelling PCA dari sklearn atau PCA from scratch **(pilih salah satu saja)**
2. Lakukan pengecekan 95% distribusi dan plotting untuk nilai explained variance ratio (explained_variance_ratio) dengan cumulative explained variance (cumulative_explained_variance)
3. Pilih nilai PC diperoleh berapa banyak kolom untuk 95% distribusi
4. Lakukan Regresi atau klasifikasi, bandingkan evaluasinya dari data test untuk model yang dengan PCA maupun tanpa PCA -> Gunakan 1 model saja dari salah satu model diatas (boleh jika ditambahkan parameter model).

## PCA From sklearn
"""

from sklearn.decomposition import PCA #proses PCA

pca = PCA()
#input n_component dicoba sebanyak kolom yang ada pada dataset
principal_components = pca.fit_transform(new_df)

# Explained variance ratio
explained_variance_ratio = pca.explained_variance_ratio_

# Cumulative explained variance
cumulative_explained_variance = np.cumsum(explained_variance_ratio)

print('Explained variance:', explained_variance_ratio)
print('Cumulative explained variance:', cumulative_explained_variance)

# Find the number of components that explain 95% of the variance
n_components_95 = np.argmax(cumulative_explained_variance >= 0.95) + 1

# Plotting the explained variance with a horizontal line at 95%
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(explained_variance_ratio) + 1), cumulative_explained_variance, marker='o', linestyle='--')
plt.axvline(x=n_components_95, color='r', linestyle='--', label=f'95% Threshold (n={n_components_95})')
plt.title('Explained Variance vs. Number of Principal Components')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.legend()
plt.grid(True)
plt.show()

#ambil kolom yang berjumlah 95% distribusi
pca = PCA(n_components=n_components_95)
principal_components = pca.fit_transform(new_df)
principal_components

# Membuat DataFrame untuk komponen utama
principal_components_df = pd.DataFrame(data=principal_components, columns=[f'PC{i}' for i in range(1, n_components_95 + 1)])

principal_components_df.head()

"""### Intepretasi

Hasil PCA menunjukkan bahwa komponen utama pertama (PC1) memiliki nilai eigen yang tertinggi, diikuti oleh PC2, PC3, dan seterusnya hingga PC5. Hal ini menandakan bahwa PC1 memiliki pengaruh terbesar dalam menjelaskan variasi data, diikuti oleh PC2 dan seterusnya, sesuai dengan urutan nilai eigen yang menurun.

Kumpulan nilai vektor di setiap baris dari PC1 hingga PC5 menunjukkan kontribusi masing-masing fitur (kolom) dalam pembentukan komponen-komponen utama tersebut. Secara umum, semakin besar nilai vektor suatu fitur pada suatu komponen utama, semakin besar kontribusi fitur tersebut terhadap komponen utama tersebut.

Dengan menggunakan PCA, dimensi kolom pada data Boston telah direduksi dari jumlah aslinya menjadi hanya lima komponen utama (PC1 hingga PC5). Reduksi dimensi ini membantu dalam mengurangi kompleksitas data dan memungkinkan kita untuk mempertahankan sebagian besar informasi dari data asli, karena lima komponen utama tersebut telah berhasil menjelaskan sekitar 95% dari variasi dalam data California.

## Regression
1. With PCA Data
2. Without PCA Data
"""

#Data dengan proses PCA
X_train_pca, X_test_pca,Y_train_pca,Y_test_pca = train_test_split(principal_components_df,y,
                                                test_size = 0.25, #75% train : 25% test
                                                random_state = 42)

# Lakukan train test split pada data yang TANPA hasil PCA
feats = ['MedInc',
 'HouseAge',
 'AveRooms',
 'AveBedrms',
 'Population',
 'AveOccup']
X = df.drop('house_price', axis=1)
y = df[['house_price']]

#Splitting
X_train, X_test,Y_train,Y_test = train_test_split(X,y,
                                                test_size = 0.25,
                                                random_state = 42)

X_train

feats

# Inisialisasi StandardScaler
scaler = StandardScaler()

# Menerapkan standarisasi ke data pelatihan (train)
X_train.loc[:, feats] = scaler.fit_transform(X_train[feats])

# Menerapkan standarisasi yang sama ke data uji (test)
X_test.loc[:, feats] = scaler.transform(X_test[feats])

X_train.describe()

"""## Modelling Tanpa PCA"""

model = LinearRegression()
model.fit(X_train, Y_train)  #ini model linearregressionnya belajar ke data BUKAN PCA

y_pred = model.predict(X_test) #Model dari hasil BUKAN PCA
y_pred_train = model.predict(X_train) #Model dari hasil BUKAN PCA

"""### Modelling Dengan PCA"""

from sklearn.linear_model import LinearRegression
model_pca = LinearRegression()
model_pca.fit(X_train_pca, Y_train_pca) #ini model linearregressionnya belajar ke data PCA

y_pred_pca = model_pca.predict(X_test_pca) #Model dari hasil PCA
y_pred_train_pca = model_pca.predict(X_train_pca) #Model dari hasil PCA

"""### Evaluasi"""

# RMSE : Itu makin kecil makin bagus
# RMSE : seberapa dekat nilai tebakan dengan nilai sebenarnya pada range max, min data target ('houseprice')
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

print('Data test (Unseen) RMSE DENGAN PCA : {}'.format(np.sqrt(mean_squared_error(Y_test_pca, y_pred_pca))))
print('Data test (Unseen) RMSE TANPA PCA : {}'.format(np.sqrt(mean_squared_error(Y_test, y_pred))))

#RMSE TANPA PCA LEBIH BAGUS DARIPADA PAKAI PCA KARENA ERRORNYA 0,72 < 0.83

from sklearn.metrics import r2_score
# R2 : Makin gede makin bagus (1-100 %)
# R2 : Makin gede maka makin linear Modelnya
print('Data test (Unseen) R-Square DENGAN PCA : {}'.format((r2_score(Y_test_pca, y_pred_pca))*100))
print('Data test (Unseen) R-Square TANPA PCA : {}'.format((r2_score(Y_test, y_pred))*100))

#R SQUARE TANPA PCA LEBIH BAGUS DARIPADA PAKAI PCA KARENA R-Squarenya 56.5 lebih besar dibandingkan dengan PCA 42.29

"""## Kesimpulan
1. Root Mean Square Error (RMSE):
        Dengan PCA: RMSE sebesar 0.8316
        Tanpa PCA: RMSE sebesar 0.7219
        Kesimpulan: Model regresi tanpa menggunakan PCA menghasilkan nilai RMSE yang lebih rendah dibandingkan dengan model regresi yang menggunakan PCA. Hal ini menunjukkan bahwa model tanpa PCA cenderung memberikan prediksi yang lebih akurat dalam memperkirakan nilai target pada data uji yang tidak terlihat.

2. R-Square:
        Dengan PCA: R-Square sebesar 42.30%
        Tanpa PCA: R-Square sebesar 56.52%
        Kesimpulan: Model regresi tanpa menggunakan PCA memiliki nilai R-Square yang lebih tinggi, yaitu sebesar 56.52%, dibandingkan dengan model regresi yang menggunakan PCA. Ini menunjukkan bahwa model tanpa PCA mampu menjelaskan variasi target yang lebih besar dalam data uji yang tidak terlihat dibandingkan dengan model yang menggunakan PCA.

**Dengan demikian, berdasarkan hasil RMSE dan R-Square, dapat disimpulkan bahwa dalam kasus ini, menggunakan regresi tanpa PCA menghasilkan model yang lebih baik daripada menggunakan regresi dengan PCA. Model tanpa PCA memberikan prediksi yang lebih akurat dan mampu menjelaskan variasi target yang lebih besar dalam data unseen.**

## THX :)
"""